{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from utils import mnist, plot_graphs, plot_mnist\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = mnist(valid=10000, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
    "                 bn=False, dropout=False, activation_fn=nn.ReLU(), stride=1):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding, stride=stride))\n",
    "        if pool_layer is not None:\n",
    "            layers.append(pool_layer)\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(size[1]))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d())\n",
    "        layers.append(activation_fn)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout())\n",
    "            layers.append(activation_fn())\n",
    "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm=False, dropout=False, optim_type='SGD', **optim_params):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self._conv1 = ConvLayer([1, 16, 4], padding=0, bn=batchnorm, stride=2, \n",
    "                                pool_layer=None, activation_fn=nn.Tanh())\n",
    "        self._conv2 = ConvLayer([16, 32, 4], padding=0, bn=batchnorm, stride=2, \n",
    "                                pool_layer=None, activation_fn=nn.Tanh())\n",
    "        self._conv3 = ConvLayer([32, 32, 3], padding=0, bn=batchnorm, stride=2, \n",
    "                                pool_layer=None, activation_fn=nn.Tanh())\n",
    "            \n",
    "        self.fc1 = FullyConnected([32*2*2, 32])\n",
    "        self.fc2 = FullyConnected([32, 32])\n",
    "        self.fc3 = FullyConnected([32, 32])\n",
    "        self.fc4 = FullyConnected([32, 10])\n",
    "        \n",
    "        self._loss = None\n",
    "        if optim_type == 'SGD':\n",
    "            self.optim = optim.SGD(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adadelta':\n",
    "            self.optim = optim.Adadelta(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'RMSProp':\n",
    "            self.optim = optim.RMSprop(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adam':\n",
    "            self.optim = optim.Adam(self.parameters(), **optim_params)\n",
    "        \n",
    "    def conv(self, x):\n",
    "        l1 = self._conv1(x)\n",
    "        l2 = self._conv2(l1)\n",
    "        l3 = self._conv3(l2)\n",
    "        return l3, l2, l1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l3 = self.conv(x)[0]\n",
    "        flatten = l3.view(-1, 32*2*2)\n",
    "        x = self.fc1(flatten)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        h = self.fc4(x)\n",
    "        return h\n",
    "    \n",
    "    def loss(self, output, target):\n",
    "        self._loss = F.cross_entropy(output, target)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FC(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2)\n",
       "    (4): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\") #\"cuda:0\" if torch.cuda.is_available() else \n",
    "print(device)\n",
    "lr = 0.0001\n",
    "prior_size = 10\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh(), flatten=False, \n",
    "                 last_fn=None, first_fn=None, device='cpu'):\n",
    "        super(FC, self).__init__()\n",
    "        layers = []\n",
    "        self.flatten = flatten\n",
    "        if first_fn is not None:\n",
    "            layers.append(first_fn)\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            layers.append(activation_fn) # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "        else: \n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        if last_fn is not None:\n",
    "            layers.append(last_fn)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0], -1)\n",
    "        if y is not None:\n",
    "            x = torch.cat([x, y], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "caae_Enc = FC([28*28, 1024, 1024, prior_size], activation_fn=nn.LeakyReLU(0.2), flatten=True, device=device)\n",
    "caae_Enc.load_state_dict(torch.load(\"./model_Enc_caae\"))\n",
    "caae_Enc.eval()\n",
    "\n",
    "aaec_Enc = FC([28*28, 1024, 1024, prior_size], activation_fn=nn.LeakyReLU(0.2), flatten=True, device=device)\n",
    "aaec_Enc.load_state_dict(torch.load(\"./model_Enc_aaec\"))\n",
    "aaec_Enc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAAE_Clf(nn.Module):\n",
    "    def __init__(self, conditional=True, batchnorm=False, dropout=False, optim_type='SGD', **optim_params):\n",
    "        super(CAAE_Clf, self).__init__()\n",
    "        self.conditional = conditional\n",
    "        self.fc1 = FullyConnected([10, 32])\n",
    "        self.fc2 = FullyConnected([32, 32])\n",
    "        self.fc3 = FullyConnected([32, 64])\n",
    "        self.fc4 = FullyConnected([64, 10])\n",
    "        \n",
    "        self._loss = None\n",
    "        if optim_type == 'SGD':\n",
    "            self.optim = optim.SGD(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adadelta':\n",
    "            self.optim = optim.Adadelta(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'RMSProp':\n",
    "            self.optim = optim.RMSprop(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adam':\n",
    "            self.optim = optim.Adam(self.parameters(), **optim_params)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.conditional:\n",
    "            x = self.fc1(caae_Enc(x))\n",
    "        else:\n",
    "            x = self.fc1(aaec_Enc(x))\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        h = self.fc4(x)\n",
    "        return h\n",
    "    \n",
    "    def loss(self, output, target):\n",
    "        self._loss = F.cross_entropy(output, target)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Clf_Img': Net(False, False, optim_type='Adam', lr=1e-4), \n",
    "          'CAAE_clf': CAAE_Clf(True, False, False, optim_type='Adam', lr=1e-4, ),\n",
    "          'AAEC_clf': CAAE_Clf(False, False, False, optim_type='Adam', lr=1e-4),\n",
    "         }\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):\n",
    "    train_size = len(train_loader.sampler)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        for model in models.values():\n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "            losses = ' '.join(['{}: {:.4f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "        losses = ' '.join(['{}: {:.4f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss,))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, loader, log=None):\n",
    "    test_size = len(loader)\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            output = {k: m(data) for k, m in models.items()}\n",
    "            for k, m in models.items():\n",
    "                test_loss[k] += m.loss(output[k], target).item() # sum up batch loss\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    report = 'Test losses: ' + ' '.join(['{}: {:.4f}'.format(k, test_loss[k]) for k in test_loss])\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k],))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLosses Clf_Img: 2.3247 CAAE_clf: 2.3209 AAEC_clf: 2.2857\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLosses Clf_Img: 1.6804 CAAE_clf: 2.2199 AAEC_clf: 2.2630\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLosses Clf_Img: 0.7444 CAAE_clf: 2.1721 AAEC_clf: 2.0943\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLosses Clf_Img: 0.4252 CAAE_clf: 2.1169 AAEC_clf: 1.8069\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLosses Clf_Img: 0.3139 CAAE_clf: 2.0108 AAEC_clf: 1.6644\n",
      "Train Epoch: 1 [50000/50000 (100%)]\tLosses Clf_Img: 0.6897 CAAE_clf: 2.1100 AAEC_clf: 1.3404\n",
      "Test losses: Clf_Img: 0.4613 CAAE_clf: 1.9714 AAEC_clf: 1.3775\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLosses Clf_Img: 0.5623 CAAE_clf: 1.9688 AAEC_clf: 1.3076\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLosses Clf_Img: 0.3729 CAAE_clf: 1.9926 AAEC_clf: 1.1743\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLosses Clf_Img: 0.1627 CAAE_clf: 1.6799 AAEC_clf: 0.8553\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLosses Clf_Img: 0.3201 CAAE_clf: 1.7616 AAEC_clf: 0.9209\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLosses Clf_Img: 0.4028 CAAE_clf: 2.0248 AAEC_clf: 0.9389\n",
      "Train Epoch: 2 [50000/50000 (100%)]\tLosses Clf_Img: 0.2182 CAAE_clf: 1.8341 AAEC_clf: 1.0826\n",
      "Test losses: Clf_Img: 0.3126 CAAE_clf: 1.8719 AAEC_clf: 1.0289\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLosses Clf_Img: 0.5842 CAAE_clf: 1.8721 AAEC_clf: 1.2967\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLosses Clf_Img: 0.2659 CAAE_clf: 1.8207 AAEC_clf: 0.9781\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLosses Clf_Img: 0.2236 CAAE_clf: 1.6069 AAEC_clf: 0.8854\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLosses Clf_Img: 0.4842 CAAE_clf: 1.8298 AAEC_clf: 1.3582\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLosses Clf_Img: 0.1468 CAAE_clf: 2.0790 AAEC_clf: 0.9059\n",
      "Train Epoch: 3 [50000/50000 (100%)]\tLosses Clf_Img: 0.3406 CAAE_clf: 2.0712 AAEC_clf: 1.1416\n",
      "Test losses: Clf_Img: 0.2413 CAAE_clf: 1.8463 AAEC_clf: 0.9634\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLosses Clf_Img: 0.4635 CAAE_clf: 1.9287 AAEC_clf: 1.0256\n",
      "Train Epoch: 4 [10000/50000 (20%)]\tLosses Clf_Img: 0.2539 CAAE_clf: 1.7618 AAEC_clf: 0.8161\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLosses Clf_Img: 0.0884 CAAE_clf: 2.0804 AAEC_clf: 0.8463\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLosses Clf_Img: 0.1872 CAAE_clf: 1.6308 AAEC_clf: 0.8064\n",
      "Train Epoch: 4 [40000/50000 (80%)]\tLosses Clf_Img: 0.1910 CAAE_clf: 2.0789 AAEC_clf: 0.9998\n",
      "Train Epoch: 4 [50000/50000 (100%)]\tLosses Clf_Img: 0.2370 CAAE_clf: 1.7702 AAEC_clf: 0.7946\n",
      "Test losses: Clf_Img: 0.1963 CAAE_clf: 1.8320 AAEC_clf: 0.9437\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLosses Clf_Img: 0.1715 CAAE_clf: 1.7558 AAEC_clf: 0.7355\n",
      "Train Epoch: 5 [10000/50000 (20%)]\tLosses Clf_Img: 0.3287 CAAE_clf: 1.7177 AAEC_clf: 1.0170\n",
      "Train Epoch: 5 [20000/50000 (40%)]\tLosses Clf_Img: 0.1792 CAAE_clf: 2.0086 AAEC_clf: 1.0468\n",
      "Train Epoch: 5 [30000/50000 (60%)]\tLosses Clf_Img: 0.2465 CAAE_clf: 1.7325 AAEC_clf: 0.9850\n",
      "Train Epoch: 5 [40000/50000 (80%)]\tLosses Clf_Img: 0.1517 CAAE_clf: 1.6656 AAEC_clf: 0.9511\n",
      "Train Epoch: 5 [50000/50000 (100%)]\tLosses Clf_Img: 0.0816 CAAE_clf: 1.8812 AAEC_clf: 0.8027\n",
      "Test losses: Clf_Img: 0.1653 CAAE_clf: 1.8205 AAEC_clf: 0.9318\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLosses Clf_Img: 0.2068 CAAE_clf: 1.9066 AAEC_clf: 0.9442\n",
      "Train Epoch: 6 [10000/50000 (20%)]\tLosses Clf_Img: 0.2291 CAAE_clf: 1.5352 AAEC_clf: 0.8051\n",
      "Train Epoch: 6 [20000/50000 (40%)]\tLosses Clf_Img: 0.2097 CAAE_clf: 1.9477 AAEC_clf: 1.0496\n",
      "Train Epoch: 6 [30000/50000 (60%)]\tLosses Clf_Img: 0.2415 CAAE_clf: 1.7464 AAEC_clf: 0.9456\n",
      "Train Epoch: 6 [40000/50000 (80%)]\tLosses Clf_Img: 0.0475 CAAE_clf: 1.5717 AAEC_clf: 0.5769\n",
      "Train Epoch: 6 [50000/50000 (100%)]\tLosses Clf_Img: 0.1106 CAAE_clf: 1.8096 AAEC_clf: 1.1534\n",
      "Test losses: Clf_Img: 0.1417 CAAE_clf: 1.8176 AAEC_clf: 0.9204\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLosses Clf_Img: 0.0450 CAAE_clf: 1.7092 AAEC_clf: 0.8928\n",
      "Train Epoch: 7 [10000/50000 (20%)]\tLosses Clf_Img: 0.1464 CAAE_clf: 1.7087 AAEC_clf: 0.8545\n",
      "Train Epoch: 7 [20000/50000 (40%)]\tLosses Clf_Img: 0.2594 CAAE_clf: 1.7864 AAEC_clf: 1.0788\n",
      "Train Epoch: 7 [30000/50000 (60%)]\tLosses Clf_Img: 0.0515 CAAE_clf: 1.6753 AAEC_clf: 0.7068\n",
      "Train Epoch: 7 [40000/50000 (80%)]\tLosses Clf_Img: 0.1659 CAAE_clf: 1.6587 AAEC_clf: 0.7323\n",
      "Train Epoch: 7 [50000/50000 (100%)]\tLosses Clf_Img: 0.1932 CAAE_clf: 1.8727 AAEC_clf: 1.0514\n",
      "Test losses: Clf_Img: 0.1274 CAAE_clf: 1.8134 AAEC_clf: 0.9111\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLosses Clf_Img: 0.1256 CAAE_clf: 1.8261 AAEC_clf: 0.9213\n",
      "Train Epoch: 8 [10000/50000 (20%)]\tLosses Clf_Img: 0.0489 CAAE_clf: 1.8549 AAEC_clf: 1.0150\n",
      "Train Epoch: 8 [20000/50000 (40%)]\tLosses Clf_Img: 0.0991 CAAE_clf: 1.7740 AAEC_clf: 0.9971\n",
      "Train Epoch: 8 [30000/50000 (60%)]\tLosses Clf_Img: 0.1770 CAAE_clf: 1.5952 AAEC_clf: 0.8881\n",
      "Train Epoch: 8 [40000/50000 (80%)]\tLosses Clf_Img: 0.1523 CAAE_clf: 2.0444 AAEC_clf: 1.2305\n",
      "Train Epoch: 8 [50000/50000 (100%)]\tLosses Clf_Img: 0.0443 CAAE_clf: 1.5660 AAEC_clf: 0.5418\n",
      "Test losses: Clf_Img: 0.1120 CAAE_clf: 1.8116 AAEC_clf: 0.9046\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLosses Clf_Img: 0.0715 CAAE_clf: 1.6830 AAEC_clf: 0.6842\n",
      "Train Epoch: 9 [10000/50000 (20%)]\tLosses Clf_Img: 0.1633 CAAE_clf: 1.9665 AAEC_clf: 1.1163\n",
      "Train Epoch: 9 [20000/50000 (40%)]\tLosses Clf_Img: 0.1010 CAAE_clf: 1.7290 AAEC_clf: 1.1477\n",
      "Train Epoch: 9 [30000/50000 (60%)]\tLosses Clf_Img: 0.0615 CAAE_clf: 1.9085 AAEC_clf: 0.9385\n",
      "Train Epoch: 9 [40000/50000 (80%)]\tLosses Clf_Img: 0.0799 CAAE_clf: 1.8764 AAEC_clf: 0.8150\n",
      "Train Epoch: 9 [50000/50000 (100%)]\tLosses Clf_Img: 0.0693 CAAE_clf: 1.9820 AAEC_clf: 1.1172\n",
      "Test losses: Clf_Img: 0.1032 CAAE_clf: 1.8099 AAEC_clf: 0.8993\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLosses Clf_Img: 0.0624 CAAE_clf: 1.6969 AAEC_clf: 1.0579\n",
      "Train Epoch: 10 [10000/50000 (20%)]\tLosses Clf_Img: 0.0394 CAAE_clf: 1.8236 AAEC_clf: 1.0493\n",
      "Train Epoch: 10 [20000/50000 (40%)]\tLosses Clf_Img: 0.0149 CAAE_clf: 1.7405 AAEC_clf: 0.8239\n",
      "Train Epoch: 10 [30000/50000 (60%)]\tLosses Clf_Img: 0.1075 CAAE_clf: 1.9906 AAEC_clf: 1.1204\n",
      "Train Epoch: 10 [40000/50000 (80%)]\tLosses Clf_Img: 0.0715 CAAE_clf: 1.6113 AAEC_clf: 0.9699\n",
      "Train Epoch: 10 [50000/50000 (100%)]\tLosses Clf_Img: 0.0604 CAAE_clf: 1.7643 AAEC_clf: 0.9135\n",
      "Test losses: Clf_Img: 0.0951 CAAE_clf: 1.8103 AAEC_clf: 0.8992\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLosses Clf_Img: 0.0734 CAAE_clf: 1.7885 AAEC_clf: 1.0666\n",
      "Train Epoch: 11 [10000/50000 (20%)]\tLosses Clf_Img: 0.1286 CAAE_clf: 1.8540 AAEC_clf: 0.8474\n",
      "Train Epoch: 11 [20000/50000 (40%)]\tLosses Clf_Img: 0.2060 CAAE_clf: 1.6568 AAEC_clf: 0.7681\n",
      "Train Epoch: 11 [30000/50000 (60%)]\tLosses Clf_Img: 0.0305 CAAE_clf: 1.7186 AAEC_clf: 0.6418\n",
      "Train Epoch: 11 [40000/50000 (80%)]\tLosses Clf_Img: 0.0438 CAAE_clf: 1.9929 AAEC_clf: 1.0066\n",
      "Train Epoch: 11 [50000/50000 (100%)]\tLosses Clf_Img: 0.0123 CAAE_clf: 1.8361 AAEC_clf: 0.8270\n",
      "Test losses: Clf_Img: 0.0889 CAAE_clf: 1.8097 AAEC_clf: 0.8995\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLosses Clf_Img: 0.0910 CAAE_clf: 1.6978 AAEC_clf: 0.7864\n",
      "Train Epoch: 12 [10000/50000 (20%)]\tLosses Clf_Img: 0.1643 CAAE_clf: 1.9201 AAEC_clf: 0.9724\n",
      "Train Epoch: 12 [20000/50000 (40%)]\tLosses Clf_Img: 0.0963 CAAE_clf: 1.7087 AAEC_clf: 0.8326\n",
      "Train Epoch: 12 [30000/50000 (60%)]\tLosses Clf_Img: 0.0466 CAAE_clf: 1.9240 AAEC_clf: 0.9969\n",
      "Train Epoch: 12 [40000/50000 (80%)]\tLosses Clf_Img: 0.0281 CAAE_clf: 1.7182 AAEC_clf: 0.9336\n",
      "Train Epoch: 12 [50000/50000 (100%)]\tLosses Clf_Img: 0.0218 CAAE_clf: 1.6963 AAEC_clf: 0.8803\n",
      "Test losses: Clf_Img: 0.0885 CAAE_clf: 1.8087 AAEC_clf: 0.8989\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLosses Clf_Img: 0.0537 CAAE_clf: 1.8639 AAEC_clf: 0.9392\n",
      "Train Epoch: 13 [10000/50000 (20%)]\tLosses Clf_Img: 0.0246 CAAE_clf: 1.7455 AAEC_clf: 1.0055\n",
      "Train Epoch: 13 [20000/50000 (40%)]\tLosses Clf_Img: 0.0620 CAAE_clf: 1.7602 AAEC_clf: 0.9167\n",
      "Train Epoch: 13 [30000/50000 (60%)]\tLosses Clf_Img: 0.3280 CAAE_clf: 1.5951 AAEC_clf: 0.9070\n",
      "Train Epoch: 13 [40000/50000 (80%)]\tLosses Clf_Img: 0.1186 CAAE_clf: 1.6912 AAEC_clf: 0.8822\n",
      "Train Epoch: 13 [50000/50000 (100%)]\tLosses Clf_Img: 0.0643 CAAE_clf: 1.6831 AAEC_clf: 0.9791\n",
      "Test losses: Clf_Img: 0.0820 CAAE_clf: 1.8082 AAEC_clf: 0.8983\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLosses Clf_Img: 0.0492 CAAE_clf: 1.8511 AAEC_clf: 1.0416\n",
      "Train Epoch: 14 [10000/50000 (20%)]\tLosses Clf_Img: 0.0807 CAAE_clf: 1.7131 AAEC_clf: 0.9654\n",
      "Train Epoch: 14 [20000/50000 (40%)]\tLosses Clf_Img: 0.0189 CAAE_clf: 1.5716 AAEC_clf: 0.8720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [30000/50000 (60%)]\tLosses Clf_Img: 0.0771 CAAE_clf: 1.7673 AAEC_clf: 0.9895\n",
      "Train Epoch: 14 [40000/50000 (80%)]\tLosses Clf_Img: 0.0821 CAAE_clf: 1.7694 AAEC_clf: 0.8242\n",
      "Train Epoch: 14 [50000/50000 (100%)]\tLosses Clf_Img: 0.0173 CAAE_clf: 1.5731 AAEC_clf: 0.8099\n",
      "Test losses: Clf_Img: 0.0779 CAAE_clf: 1.8093 AAEC_clf: 0.8970\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLosses Clf_Img: 0.0365 CAAE_clf: 1.7673 AAEC_clf: 0.7190\n",
      "Train Epoch: 15 [10000/50000 (20%)]\tLosses Clf_Img: 0.0325 CAAE_clf: 1.9285 AAEC_clf: 0.9229\n",
      "Train Epoch: 15 [20000/50000 (40%)]\tLosses Clf_Img: 0.0304 CAAE_clf: 1.7003 AAEC_clf: 0.9028\n",
      "Train Epoch: 15 [30000/50000 (60%)]\tLosses Clf_Img: 0.0108 CAAE_clf: 1.6557 AAEC_clf: 0.6148\n",
      "Train Epoch: 15 [40000/50000 (80%)]\tLosses Clf_Img: 0.0079 CAAE_clf: 1.6831 AAEC_clf: 0.8301\n",
      "Train Epoch: 15 [50000/50000 (100%)]\tLosses Clf_Img: 0.0182 CAAE_clf: 1.7086 AAEC_clf: 0.8852\n",
      "Test losses: Clf_Img: 0.0789 CAAE_clf: 1.8091 AAEC_clf: 0.9002\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLosses Clf_Img: 0.0242 CAAE_clf: 1.6761 AAEC_clf: 0.7399\n",
      "Train Epoch: 16 [10000/50000 (20%)]\tLosses Clf_Img: 0.0534 CAAE_clf: 1.7148 AAEC_clf: 0.8628\n",
      "Train Epoch: 16 [20000/50000 (40%)]\tLosses Clf_Img: 0.0863 CAAE_clf: 1.9055 AAEC_clf: 0.9389\n",
      "Train Epoch: 16 [30000/50000 (60%)]\tLosses Clf_Img: 0.0470 CAAE_clf: 1.7180 AAEC_clf: 0.7871\n",
      "Train Epoch: 16 [40000/50000 (80%)]\tLosses Clf_Img: 0.0135 CAAE_clf: 1.8148 AAEC_clf: 0.7801\n",
      "Train Epoch: 16 [50000/50000 (100%)]\tLosses Clf_Img: 0.0190 CAAE_clf: 1.9715 AAEC_clf: 1.1324\n",
      "Test losses: Clf_Img: 0.0724 CAAE_clf: 1.8085 AAEC_clf: 0.8968\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLosses Clf_Img: 0.0529 CAAE_clf: 1.5786 AAEC_clf: 0.7664\n",
      "Train Epoch: 17 [10000/50000 (20%)]\tLosses Clf_Img: 0.0405 CAAE_clf: 1.6996 AAEC_clf: 0.8020\n",
      "Train Epoch: 17 [20000/50000 (40%)]\tLosses Clf_Img: 0.1374 CAAE_clf: 1.7551 AAEC_clf: 1.0582\n",
      "Train Epoch: 17 [30000/50000 (60%)]\tLosses Clf_Img: 0.0080 CAAE_clf: 1.9116 AAEC_clf: 0.8700\n",
      "Train Epoch: 17 [40000/50000 (80%)]\tLosses Clf_Img: 0.0283 CAAE_clf: 1.7974 AAEC_clf: 0.8714\n",
      "Train Epoch: 17 [50000/50000 (100%)]\tLosses Clf_Img: 0.1319 CAAE_clf: 1.8202 AAEC_clf: 1.2149\n",
      "Test losses: Clf_Img: 0.0719 CAAE_clf: 1.8069 AAEC_clf: 0.8986\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLosses Clf_Img: 0.0681 CAAE_clf: 1.7170 AAEC_clf: 0.9125\n",
      "Train Epoch: 18 [10000/50000 (20%)]\tLosses Clf_Img: 0.0628 CAAE_clf: 2.1871 AAEC_clf: 1.1411\n",
      "Train Epoch: 18 [20000/50000 (40%)]\tLosses Clf_Img: 0.0306 CAAE_clf: 1.7686 AAEC_clf: 0.6047\n",
      "Train Epoch: 18 [30000/50000 (60%)]\tLosses Clf_Img: 0.0525 CAAE_clf: 1.9100 AAEC_clf: 0.9358\n",
      "Train Epoch: 18 [40000/50000 (80%)]\tLosses Clf_Img: 0.0082 CAAE_clf: 1.8768 AAEC_clf: 0.8756\n",
      "Train Epoch: 18 [50000/50000 (100%)]\tLosses Clf_Img: 0.0136 CAAE_clf: 1.6131 AAEC_clf: 0.8333\n",
      "Test losses: Clf_Img: 0.0684 CAAE_clf: 1.8074 AAEC_clf: 0.8983\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLosses Clf_Img: 0.0654 CAAE_clf: 1.5792 AAEC_clf: 0.9781\n",
      "Train Epoch: 19 [10000/50000 (20%)]\tLosses Clf_Img: 0.0530 CAAE_clf: 1.9960 AAEC_clf: 1.0348\n",
      "Train Epoch: 19 [20000/50000 (40%)]\tLosses Clf_Img: 0.0421 CAAE_clf: 1.7811 AAEC_clf: 1.0103\n",
      "Train Epoch: 19 [30000/50000 (60%)]\tLosses Clf_Img: 0.0056 CAAE_clf: 1.9185 AAEC_clf: 0.7526\n",
      "Train Epoch: 19 [40000/50000 (80%)]\tLosses Clf_Img: 0.0337 CAAE_clf: 1.6496 AAEC_clf: 0.8473\n",
      "Train Epoch: 19 [50000/50000 (100%)]\tLosses Clf_Img: 0.0235 CAAE_clf: 1.7592 AAEC_clf: 0.8343\n",
      "Test losses: Clf_Img: 0.0669 CAAE_clf: 1.8086 AAEC_clf: 0.8975\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLosses Clf_Img: 0.0117 CAAE_clf: 1.6314 AAEC_clf: 0.8672\n",
      "Train Epoch: 20 [10000/50000 (20%)]\tLosses Clf_Img: 0.0131 CAAE_clf: 1.8844 AAEC_clf: 0.9548\n",
      "Train Epoch: 20 [20000/50000 (40%)]\tLosses Clf_Img: 0.0420 CAAE_clf: 1.7122 AAEC_clf: 0.8100\n",
      "Train Epoch: 20 [30000/50000 (60%)]\tLosses Clf_Img: 0.0076 CAAE_clf: 1.5751 AAEC_clf: 0.7008\n",
      "Train Epoch: 20 [40000/50000 (80%)]\tLosses Clf_Img: 0.1337 CAAE_clf: 1.9117 AAEC_clf: 0.8569\n",
      "Train Epoch: 20 [50000/50000 (100%)]\tLosses Clf_Img: 0.0859 CAAE_clf: 1.7002 AAEC_clf: 0.7626\n",
      "Test losses: Clf_Img: 0.0670 CAAE_clf: 1.8076 AAEC_clf: 0.8976\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLosses Clf_Img: 0.0154 CAAE_clf: 1.7710 AAEC_clf: 0.6994\n",
      "Train Epoch: 21 [10000/50000 (20%)]\tLosses Clf_Img: 0.0062 CAAE_clf: 1.9281 AAEC_clf: 1.0605\n",
      "Train Epoch: 21 [20000/50000 (40%)]\tLosses Clf_Img: 0.2046 CAAE_clf: 1.7841 AAEC_clf: 1.0226\n",
      "Train Epoch: 21 [30000/50000 (60%)]\tLosses Clf_Img: 0.0155 CAAE_clf: 1.5265 AAEC_clf: 0.8895\n",
      "Train Epoch: 21 [40000/50000 (80%)]\tLosses Clf_Img: 0.0071 CAAE_clf: 1.7455 AAEC_clf: 1.0729\n",
      "Train Epoch: 21 [50000/50000 (100%)]\tLosses Clf_Img: 0.0093 CAAE_clf: 1.6149 AAEC_clf: 0.8233\n",
      "Test losses: Clf_Img: 0.0678 CAAE_clf: 1.8071 AAEC_clf: 0.8973\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLosses Clf_Img: 0.0132 CAAE_clf: 1.8683 AAEC_clf: 0.9814\n",
      "Train Epoch: 22 [10000/50000 (20%)]\tLosses Clf_Img: 0.0643 CAAE_clf: 1.8802 AAEC_clf: 1.0064\n",
      "Train Epoch: 22 [20000/50000 (40%)]\tLosses Clf_Img: 0.0150 CAAE_clf: 1.8482 AAEC_clf: 0.8325\n",
      "Train Epoch: 22 [30000/50000 (60%)]\tLosses Clf_Img: 0.1487 CAAE_clf: 1.4828 AAEC_clf: 0.8413\n",
      "Train Epoch: 22 [40000/50000 (80%)]\tLosses Clf_Img: 0.0195 CAAE_clf: 1.9099 AAEC_clf: 1.0046\n",
      "Train Epoch: 22 [50000/50000 (100%)]\tLosses Clf_Img: 0.1260 CAAE_clf: 1.9750 AAEC_clf: 0.9914\n",
      "Test losses: Clf_Img: 0.0658 CAAE_clf: 1.8062 AAEC_clf: 0.8972\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLosses Clf_Img: 0.0827 CAAE_clf: 1.7963 AAEC_clf: 0.9648\n",
      "Train Epoch: 23 [10000/50000 (20%)]\tLosses Clf_Img: 0.0226 CAAE_clf: 1.6479 AAEC_clf: 0.7033\n",
      "Train Epoch: 23 [20000/50000 (40%)]\tLosses Clf_Img: 0.0917 CAAE_clf: 1.8680 AAEC_clf: 0.6312\n",
      "Train Epoch: 23 [30000/50000 (60%)]\tLosses Clf_Img: 0.0055 CAAE_clf: 1.6179 AAEC_clf: 0.6713\n",
      "Train Epoch: 23 [40000/50000 (80%)]\tLosses Clf_Img: 0.0336 CAAE_clf: 1.7208 AAEC_clf: 0.9534\n",
      "Train Epoch: 23 [50000/50000 (100%)]\tLosses Clf_Img: 0.0061 CAAE_clf: 1.7892 AAEC_clf: 0.8483\n",
      "Test losses: Clf_Img: 0.0658 CAAE_clf: 1.8065 AAEC_clf: 0.8964\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLosses Clf_Img: 0.0260 CAAE_clf: 1.9449 AAEC_clf: 0.9640\n",
      "Train Epoch: 24 [10000/50000 (20%)]\tLosses Clf_Img: 0.0541 CAAE_clf: 1.8563 AAEC_clf: 1.0100\n",
      "Train Epoch: 24 [20000/50000 (40%)]\tLosses Clf_Img: 0.0031 CAAE_clf: 1.6595 AAEC_clf: 0.7486\n",
      "Train Epoch: 24 [30000/50000 (60%)]\tLosses Clf_Img: 0.0141 CAAE_clf: 1.8860 AAEC_clf: 0.7765\n",
      "Train Epoch: 24 [40000/50000 (80%)]\tLosses Clf_Img: 0.0278 CAAE_clf: 1.8579 AAEC_clf: 0.6355\n",
      "Train Epoch: 24 [50000/50000 (100%)]\tLosses Clf_Img: 0.0256 CAAE_clf: 1.7805 AAEC_clf: 0.7742\n",
      "Test losses: Clf_Img: 0.0671 CAAE_clf: 1.8065 AAEC_clf: 0.8964\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLosses Clf_Img: 0.1368 CAAE_clf: 2.0162 AAEC_clf: 0.9269\n",
      "Train Epoch: 25 [10000/50000 (20%)]\tLosses Clf_Img: 0.0209 CAAE_clf: 1.9027 AAEC_clf: 0.6493\n",
      "Train Epoch: 25 [20000/50000 (40%)]\tLosses Clf_Img: 0.0096 CAAE_clf: 1.9463 AAEC_clf: 0.8384\n",
      "Train Epoch: 25 [30000/50000 (60%)]\tLosses Clf_Img: 0.0495 CAAE_clf: 2.0173 AAEC_clf: 0.9534\n",
      "Train Epoch: 25 [40000/50000 (80%)]\tLosses Clf_Img: 0.0311 CAAE_clf: 1.9983 AAEC_clf: 1.0372\n",
      "Train Epoch: 25 [50000/50000 (100%)]\tLosses Clf_Img: 0.0714 CAAE_clf: 1.7861 AAEC_clf: 1.0745\n",
      "Test losses: Clf_Img: 0.0665 CAAE_clf: 1.8071 AAEC_clf: 0.8969\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLosses Clf_Img: 0.0727 CAAE_clf: 1.7524 AAEC_clf: 0.8648\n",
      "Train Epoch: 26 [10000/50000 (20%)]\tLosses Clf_Img: 0.0264 CAAE_clf: 1.9648 AAEC_clf: 1.1664\n",
      "Train Epoch: 26 [20000/50000 (40%)]\tLosses Clf_Img: 0.0382 CAAE_clf: 2.0326 AAEC_clf: 1.0498\n",
      "Train Epoch: 26 [30000/50000 (60%)]\tLosses Clf_Img: 0.0628 CAAE_clf: 1.7762 AAEC_clf: 0.7417\n",
      "Train Epoch: 26 [40000/50000 (80%)]\tLosses Clf_Img: 0.0275 CAAE_clf: 1.4832 AAEC_clf: 0.7618\n",
      "Train Epoch: 26 [50000/50000 (100%)]\tLosses Clf_Img: 0.0163 CAAE_clf: 1.8889 AAEC_clf: 1.0481\n",
      "Test losses: Clf_Img: 0.0638 CAAE_clf: 1.8066 AAEC_clf: 0.8971\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLosses Clf_Img: 0.0016 CAAE_clf: 1.7615 AAEC_clf: 0.8064\n",
      "Train Epoch: 27 [10000/50000 (20%)]\tLosses Clf_Img: 0.0474 CAAE_clf: 1.7041 AAEC_clf: 0.9671\n",
      "Train Epoch: 27 [20000/50000 (40%)]\tLosses Clf_Img: 0.0016 CAAE_clf: 1.5900 AAEC_clf: 0.9594\n",
      "Train Epoch: 27 [30000/50000 (60%)]\tLosses Clf_Img: 0.0064 CAAE_clf: 1.8174 AAEC_clf: 0.7734\n",
      "Train Epoch: 27 [40000/50000 (80%)]\tLosses Clf_Img: 0.0141 CAAE_clf: 1.7713 AAEC_clf: 0.8979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 27 [50000/50000 (100%)]\tLosses Clf_Img: 0.0129 CAAE_clf: 1.6709 AAEC_clf: 0.7357\n",
      "Test losses: Clf_Img: 0.0666 CAAE_clf: 1.8057 AAEC_clf: 0.8969\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLosses Clf_Img: 0.0049 CAAE_clf: 1.5469 AAEC_clf: 0.8301\n",
      "Train Epoch: 28 [10000/50000 (20%)]\tLosses Clf_Img: 0.0017 CAAE_clf: 1.8434 AAEC_clf: 1.2472\n",
      "Train Epoch: 28 [20000/50000 (40%)]\tLosses Clf_Img: 0.0057 CAAE_clf: 2.1220 AAEC_clf: 1.1655\n",
      "Train Epoch: 28 [30000/50000 (60%)]\tLosses Clf_Img: 0.0061 CAAE_clf: 1.7388 AAEC_clf: 1.1683\n",
      "Train Epoch: 28 [40000/50000 (80%)]\tLosses Clf_Img: 0.0583 CAAE_clf: 1.6886 AAEC_clf: 1.0106\n",
      "Train Epoch: 28 [50000/50000 (100%)]\tLosses Clf_Img: 0.0063 CAAE_clf: 1.8077 AAEC_clf: 0.8363\n",
      "Test losses: Clf_Img: 0.0659 CAAE_clf: 1.8065 AAEC_clf: 0.8972\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLosses Clf_Img: 0.0348 CAAE_clf: 1.7223 AAEC_clf: 0.9397\n",
      "Train Epoch: 29 [10000/50000 (20%)]\tLosses Clf_Img: 0.0214 CAAE_clf: 1.7845 AAEC_clf: 0.9464\n",
      "Train Epoch: 29 [20000/50000 (40%)]\tLosses Clf_Img: 0.3220 CAAE_clf: 2.0239 AAEC_clf: 0.8910\n",
      "Train Epoch: 29 [30000/50000 (60%)]\tLosses Clf_Img: 0.0298 CAAE_clf: 1.9117 AAEC_clf: 0.7697\n",
      "Train Epoch: 29 [40000/50000 (80%)]\tLosses Clf_Img: 0.0237 CAAE_clf: 1.5926 AAEC_clf: 1.0676\n",
      "Train Epoch: 29 [50000/50000 (100%)]\tLosses Clf_Img: 0.0259 CAAE_clf: 1.5966 AAEC_clf: 0.7746\n",
      "Test losses: Clf_Img: 0.0663 CAAE_clf: 1.8076 AAEC_clf: 0.8997\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLosses Clf_Img: 0.0083 CAAE_clf: 1.4818 AAEC_clf: 0.5513\n",
      "Train Epoch: 30 [10000/50000 (20%)]\tLosses Clf_Img: 0.0148 CAAE_clf: 1.9733 AAEC_clf: 0.8743\n",
      "Train Epoch: 30 [20000/50000 (40%)]\tLosses Clf_Img: 0.0095 CAAE_clf: 1.9161 AAEC_clf: 1.3310\n",
      "Train Epoch: 30 [30000/50000 (60%)]\tLosses Clf_Img: 0.0571 CAAE_clf: 1.7778 AAEC_clf: 0.9379\n",
      "Train Epoch: 30 [40000/50000 (80%)]\tLosses Clf_Img: 0.0371 CAAE_clf: 1.6678 AAEC_clf: 0.9635\n",
      "Train Epoch: 30 [50000/50000 (100%)]\tLosses Clf_Img: 0.1102 CAAE_clf: 1.8221 AAEC_clf: 0.8924\n",
      "Test losses: Clf_Img: 0.0660 CAAE_clf: 1.8065 AAEC_clf: 0.8971\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 31):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, valid_loader, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
