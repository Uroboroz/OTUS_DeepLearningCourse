{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch import autograd\n",
    "\n",
    "from utils_2 import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(x, y):\n",
    "    m = y.shape[0]\n",
    "    exps = (x - torch.max(x)).exp()\n",
    "    p = exps / torch.sum(exps)\n",
    "    loss = -1*torch.log(p[range(m),y])\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.optim = optim.Adam(self.parameters(), lr=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = CrossEntropy(output, target)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        self.optim = optim.Adam(self.parameters(), lr=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.nll_loss(output, target)\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, models):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data, target\n",
    "        for model in models:\n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader))\n",
    "            losses = ' '.join(['{}: {:.6f}'.format(i, m._loss.item()) for i, m in enumerate(models)])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader))\n",
    "        losses = ' '.join(['{}: {:.6f}'.format(i, m._loss.item()) for i, m in enumerate(models)])\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [Net(), Net_()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, len(test_loader.dataset), p)\n",
    "line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "def test(models):\n",
    "    test_loss = [0]*len(models)\n",
    "    correct = [0]*len(models)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = [m(data) for m in models]\n",
    "            for i, m in enumerate(models):\n",
    "                test_loss[i] += m.loss(output[i], target, size_average=False).item() # sum up batch loss\n",
    "                pred = output[i].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct[i] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        test_loss[i] /= len(test_loader.dataset)\n",
    "    correct_pct = [100. * c / len(test_loader.dataset) for c in correct]\n",
    "    lines = '\\n'.join([line(i, test_loss[i], correct[i], correct_pct[i]) for i in range(len(models))]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    \n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLosses 0: 6.283264 1: 2.384179\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLosses 0: 4.475141 1: 0.505654\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLosses 0: 4.140670 1: 0.344053\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLosses 0: 4.193922 1: 0.361607\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLosses 0: 4.174845 1: 0.181345\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLosses 0: 4.127354 1: 0.235186\n",
      "Train Epoch: 1 [60000/60000 (100%)]\tLosses 0: 4.157121 1: 0.266916\n",
      "Test set:\n",
      "0: Loss: 0.0830\tAccuracy: 9379/10000 (93%)\n",
      "1: Loss: 0.0043\tAccuracy: 9319/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLosses 0: 4.206769 1: 0.377788\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLosses 0: 3.995508 1: 0.123853\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLosses 0: 4.059421 1: 0.204875\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLosses 0: 4.089608 1: 0.197366\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLosses 0: 4.086877 1: 0.242751\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLosses 0: 4.043405 1: 0.115743\n",
      "Train Epoch: 2 [60000/60000 (100%)]\tLosses 0: 4.091091 1: 0.222553\n",
      "Test set:\n",
      "0: Loss: 0.0822\tAccuracy: 9454/10000 (94%)\n",
      "1: Loss: 0.0043\tAccuracy: 9390/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLosses 0: 4.101248 1: 0.117732\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLosses 0: 4.183615 1: 0.136487\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLosses 0: 4.064494 1: 0.122415\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLosses 0: 4.205910 1: 0.189553\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLosses 0: 4.033314 1: 0.061248\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLosses 0: 4.021004 1: 0.118070\n",
      "Train Epoch: 3 [60000/60000 (100%)]\tLosses 0: 4.051093 1: 0.132095\n",
      "Test set:\n",
      "0: Loss: 0.0819\tAccuracy: 9530/10000 (95%)\n",
      "1: Loss: 0.0039\tAccuracy: 9408/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLosses 0: 3.995578 1: 0.144862\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLosses 0: 4.105255 1: 0.248485\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLosses 0: 4.078163 1: 0.089634\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLosses 0: 4.024445 1: 0.048522\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLosses 0: 4.005839 1: 0.240986\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLosses 0: 4.012243 1: 0.097826\n",
      "Train Epoch: 4 [60000/60000 (100%)]\tLosses 0: 4.191096 1: 0.121660\n",
      "Test set:\n",
      "0: Loss: 0.0815\tAccuracy: 9551/10000 (95%)\n",
      "1: Loss: 0.0035\tAccuracy: 9415/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLosses 0: 4.060778 1: 0.102170\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLosses 0: 4.198163 1: 0.238078\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLosses 0: 4.011681 1: 0.064702\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLosses 0: 4.003244 1: 0.106196\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLosses 0: 4.192916 1: 0.334459\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLosses 0: 4.007517 1: 0.143361\n",
      "Train Epoch: 5 [60000/60000 (100%)]\tLosses 0: 4.205585 1: 0.368582\n",
      "Test set:\n",
      "0: Loss: 0.0813\tAccuracy: 9594/10000 (95%)\n",
      "1: Loss: 0.0040\tAccuracy: 9400/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLosses 0: 4.062575 1: 0.118945\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLosses 0: 4.117743 1: 0.099475\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLosses 0: 4.021871 1: 0.095811\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLosses 0: 4.120274 1: 0.128978\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLosses 0: 3.981819 1: 0.115721\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLosses 0: 4.206013 1: 0.162781\n",
      "Train Epoch: 6 [60000/60000 (100%)]\tLosses 0: 4.084497 1: 0.093399\n",
      "Test set:\n",
      "0: Loss: 0.0815\tAccuracy: 9558/10000 (95%)\n",
      "1: Loss: 0.0031\tAccuracy: 9514/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLosses 0: 4.051406 1: 0.076910\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLosses 0: 4.006583 1: 0.101316\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLosses 0: 3.987125 1: 0.114683\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLosses 0: 4.016932 1: 0.294356\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLosses 0: 4.071032 1: 0.138168\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLosses 0: 4.005084 1: 0.060851\n",
      "Train Epoch: 7 [60000/60000 (100%)]\tLosses 0: 4.241404 1: 0.418585\n",
      "Test set:\n",
      "0: Loss: 0.0815\tAccuracy: 9557/10000 (95%)\n",
      "1: Loss: 0.0033\tAccuracy: 9454/10000 (94%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLosses 0: 4.102745 1: 0.154673\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLosses 0: 4.098587 1: 0.110040\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLosses 0: 3.962560 1: 0.160971\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLosses 0: 4.031042 1: 0.054399\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLosses 0: 3.970328 1: 0.111309\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLosses 0: 4.072987 1: 0.118977\n",
      "Train Epoch: 8 [60000/60000 (100%)]\tLosses 0: 3.965066 1: 0.127566\n",
      "Test set:\n",
      "0: Loss: 0.0815\tAccuracy: 9527/10000 (95%)\n",
      "1: Loss: 0.0034\tAccuracy: 9467/10000 (94%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLosses 0: 4.092797 1: 0.133011\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLosses 0: 4.003004 1: 0.183346\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLosses 0: 3.994260 1: 0.075014\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLosses 0: 3.993256 1: 0.070228\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 21):\n",
    "    train(epoch, models)\n",
    "    test(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
